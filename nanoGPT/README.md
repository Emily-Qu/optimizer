# MiniNet-OptBench

æœ¬ä»“åº“å¿«é€Ÿä¸Šæ‰‹ä¸”è‡ªåŠ¨åŒ–ç¨‹åº¦è¾ƒé«˜ï¼Œç”¨äºè®­ç»ƒ/å¾®è°ƒä¸­ç­‰è§„æ¨¡GPTç­‰æ¨¡å‹å¹¶è¿›è¡Œä¼˜åŒ–å™¨åŸºå‡†æµ‹è¯•ã€‚
---

## å®‰è£…åŠé…ç½®ç¯å¢ƒ

ä½¿ç”¨ Conda åˆ›å»ºå¹¶æ¿€æ´»ç¯å¢ƒï¼š

```sh
conda env create -f environment.yml
conda activate MiniNet-OptBench
````

---

## å®‰è£…åŠé…ç½®ç¯å¢ƒ

è¿è¡Œæä¾›çš„shellè„šæœ¬æ¥å¯¹ä¼˜åŒ–å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼š

```sh
bash generate_loss_table.sh
```

è¯¥è„šæœ¬å°†è‡ªåŠ¨å®Œæˆï¼š

æ•°æ®é¢„å¤„ç†ä¸ç¼“å­˜ç®¡ç†ï¼ˆç”Ÿæˆ data/{dataset}/train.bin å’Œ data/{dataset}/val.binï¼‰
å„ä¼˜åŒ–å™¨çš„è¶…å‚æ•°è®¾ç½®
ç»“æœè‡ªåŠ¨æ±‡æ€»ä¸è®°å½•ï¼ˆä¿å­˜è‡³ results/bench_runs.csvï¼‰
---

## æ ¸å¿ƒåŠŸèƒ½é…ç½®

### æ•°æ®é›†é€‰æ‹©

```bash
DATASET=`shakespeare`
```

æ”¯æŒçš„æ•°æ®é›†ï¼š

* `shakespeare`
* `shakespeare_char`
* `openwebtext`
* `openwebtext-small`(ä»openwebtextæ•°æ®é›†é‡‡æ ·3%åçš„å°å‹æ•°æ®é›†)
---

### ä¼˜åŒ–å™¨é…ç½®

* **Adam / AdamW ç³»åˆ—**

```bash
ADAM_LRS="0.0001 0.0003 0.001"   
ADAMW_LRS="0.0001 0.0003 0.001"
```

---




## ğŸ”§ How to Run Different Optimizers

The script defines grids for each optimizer family:

* **Adam / AdamW**

  ```bash
  ADAM_LRS="0.0003 0.001" 
  ADAMW_LRS="0.0003 0.001"
  ```

* **SGD**

  ```bash
  SGD_LRS="0.01 0.05" 
  SGD_MOMS="0.0 0.9"
  ```

* **Muon**

  ```bash
  MUON_LRS="0.01 0.02" 
  MUON_MOMS="0.95 0.99"
  ```

* **Muon + Aux Adam (hybrid)**

  ```bash
  OPT_LIST='("MUON_WITH_AUX_ADAM")'
  ```

Each run is tagged with a descriptive `RUN_NAME` like `MUON_lr0.01_m0.95` so you can track curves.

---

## ğŸ“Š Visualizing Results

Once training runs are done, you can generate loss-vs-iteration plots:

```bash
python viz_curves.py --csv results/<DATASET>_runs.csv --dataset <DATASET> --split val_eval --outdir results/figs
```

This will produce:

* Best run per optimizer
* All runs overlaid
* Separate plots per optimizer

---

## ğŸ™ Acknowledgements

Thanks to **Cardinal Operations** for funding, which made this repository possible.